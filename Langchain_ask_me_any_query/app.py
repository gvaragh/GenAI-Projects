import streamlit as st
import requests
from langchain_opentutorial import set_env
from langchain_core.prompts import PromptTemplate
from langchain_openai.chat_models import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser

st.title("GenAI Project using Langchain chatModel !!!")

# Configuration File for Managing API Key as an Environment Variable
# Load API KEY Information
load_dotenv(override=True)

model = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

output_parser = (
    StrOutputParser()
)  # Directly returns the model's response as a string without modification.

query =st.text_input("Enter the topic of your interest ..")

# Create the prompt as a `PromptTemplate` object.
prompt = PromptTemplate.from_template("Please explain {topic} in simple terms.")

# Combine the prompt and model into a chain
chain = prompt | model | output_parser # The `output_parser` in the code snippet provided is an instance of the
# `StrOutputParser` class. Its purpose is to parse and process the output
# generated by the language model (in this case, the ChatOpenAI model) and
# return it as a string without any modification.
# The `output_parser` in the code snippet provided is an instance of the
# `StrOutputParser` class. Its purpose is to parse and process the output
# generated by the language model (in this case, the ChatOpenAI model) and
# return the model's response as a string without any modification.


if st.button("Submit"):
    #url = "https://www.w3schools.com/python/demopage.php"
    # Set the topic in the `input` dictionary to 'The Principles of Learning in Artificial Intelligence Models'.
    input = {"topic": query}
    #result = requests.post(url, data=myobj)
    # Request for Streaming Output
    answer = chain.invoke(input)

    # Streaming Output
    #for token in answer:
        #print(token, end="", flush=True)
    st.write(answer)